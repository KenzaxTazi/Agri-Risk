{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add the locations of the data folders here, and the crop file locations\n",
    "seasonal_data_folder = '../data/seasonal_era_agriclimatic/'\n",
    "ten_day_data_folder = '../data/10-day-data/'\n",
    "crop_data_2005_file = '../data/spam2005v3r2_global_yield/spam2005V3r2_global_Y_TA.csv'\n",
    "crop_data_2010_file = '../data/spam2010v1r1_global_yield/spam2010V1r1_global_Y_TA.csv'\n",
    "\n",
    "# Leave this, it just stores the names of the ten day feature files\n",
    "ten_day_feature_files = {\n",
    "    'BEDD': 'BEDD_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'FD': 'FD_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'R20mm': 'R20mm_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'R10mm': 'R10mm_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'ID': 'ID_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TG': 'TG_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TN': 'TN_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'DTR': 'DTR_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'RR1': 'RR1_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'RR': 'RR_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'SDII': 'SDII_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'SU': 'SU_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TG': 'TG_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TNn': 'TNn_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TR': 'TR_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TX': 'TX_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TXn': 'TXn_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc',\n",
    "    'TXx': 'TXx_C3S-glob-agric_WFDEI_hist_dek_19810101-20101231_v1.nc'\n",
    "}\n",
    "\n",
    "# I noticed the CSU file is actually the same as the CSDI file, so I havent included it\n",
    "seasonal_feature_files = {\n",
    "    'CDD': 'CDD_C3S-glob-agric_WFDEI_hist_season_19810101-20101231_v1.nc',\n",
    "    'CFD': 'CFD_C3S-glob-agric_WFDEI_hist_season_19810101-20101231_v1.nc',\n",
    "    'CWD': 'CWD_C3S-glob-agric_WFDEI_hist_season_19810101-20101231_v1.nc',\n",
    "    'WW': 'WW_C3S-glob-agric_WFDEI_hist_season_19810101-20101231_v1.nc',\n",
    "    'WSDI': 'WSDI_C3S-glob-agric_WFDEI_hist_season_19810101-20101231_v1.nc',\n",
    "    'CSDI': 'CSDI_C3S-glob-agric_WFDEI_hist_season_19810101-20101231_v1.nc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create crop data file\n",
    "crops = ['maize']\n",
    "crop_data_2005 = pd.read_csv(crop_data_2005_file,  encoding = \"ISO-8859-1\")\n",
    "crop_data_2010 = pd.read_csv(crop_data_2010_file,  encoding = \"ISO-8859-1\")\n",
    "\n",
    "merged = crop_data_2010.merge(crop_data_2005, how='inner', on='alloc_key', suffixes=['_2010', '_2005'])\n",
    "kept_columns = ['alloc_key', 'x', 'y', 'iso3_2010']\n",
    "for crop in crops:\n",
    "    kept_columns += [f'{crop[0:4]}_a_2010', f'{crop[0:4]}_a_2005']\n",
    "merged = merged[kept_columns]\n",
    "for crop in crops:\n",
    "    merged = merged.rename(columns={\n",
    "    f'{crop[0:4]}_a_2010': f'{crop}_a_2010',\n",
    "    f'{crop[0:4]}_a_2005': f'{crop}_a_2005',\n",
    "})\n",
    "    \n",
    "merged = merged.rename(columns={\n",
    "    'iso3_2010': 'iso3',\n",
    "    'x': 'lon',\n",
    "    'y': 'lat'\n",
    "})\n",
    "\n",
    "del crop_data_2005\n",
    "del crop_data_2010\n",
    "data = merged.dropna()\n",
    "del merged\n",
    "\n",
    "# Only include non zero yields\n",
    "data = data[data['maize_a_2005'] > 0]\n",
    "data = data[data['maize_a_2010'] > 0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes columns which compute the nearest the x y coordinates in the climate data.\n",
    "# This massively speeds up the computation of the climatic indicators.\n",
    "data['nearest_lat'] = 0\n",
    "data['bool'] = 0\n",
    "data['bool'] = ((data['lat'] % 1) <= 0.5).astype(float)\n",
    "data['nearest_lat'] =  data['bool'] * (data['lat'].astype(int) + 0.25) + (1 - data['bool']) * (data['lat'].astype(int) + 0.75)\n",
    "\n",
    "data['nearest_lon'] = 0\n",
    "data['bool'] = 0\n",
    "data['bool'] = ((data['lon'] % 1) <= 0.5).astype(float)\n",
    "data['nearest_lon'] =  data['bool'] * (data['lon'].astype(int) + 0.25) + (1 - data['bool']) * (data['lon'].astype(int) + 0.75)\n",
    "data = data.drop(columns='bool')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the seasonal features\n",
    "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "years = ['2010', '2005']\n",
    "seasonal_features = [\n",
    "    'CDD',\n",
    "    'CFD',\n",
    "    'CWD',\n",
    "    'WW',\n",
    "    'WSDI',\n",
    "    'CSDI'\n",
    "]\n",
    "quarter_time_mapping = {\n",
    "    'Q1': '01-16',\n",
    "    'Q2': '04-16',\n",
    "    'Q3': '07-16',\n",
    "    'Q4': '10-16'\n",
    "}\n",
    "\n",
    "lats_ = xr.DataArray(list(data['nearest_lat'].values), dims='z')\n",
    "lons_ = xr.DataArray(list(data['nearest_lon'].values), dims='z')\n",
    "\n",
    "for feature in tqdm(seasonal_features):\n",
    "    for year in years:\n",
    "        with xr.open_dataset(seasonal_data_folder + seasonal_feature_files[feature]) as ds:\n",
    "            feature_data = ds.load()\n",
    "        for quarter in quarters:\n",
    "            feature_name = f'{feature}-{quarter}-{year}'\n",
    "            time = f'{year}-{quarter_time_mapping[quarter]}'\n",
    "            time_data = feature_data.sel(time=time).squeeze().sel(lat=lats_).sel(lon=lons_)\n",
    "            data[feature_name] = getattr(time_data, feature)\n",
    "            del time_data\n",
    "    del feature_data\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 10 day features\n",
    "\n",
    "# Choose the features you want at the top of the notebook based on the ten_day_feature_files\n",
    "# dict.\n",
    "days = ['05', '15', '25']\n",
    "years = ['2010', '2005']\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "ten_day_features = [\n",
    "    'BEDD',\n",
    "    'FD',\n",
    "    'R20mm',\n",
    "    'RR1',\n",
    "    'DTR'\n",
    "]\n",
    "\n",
    "lats_ = xr.DataArray(list(data['nearest_lat'].values), dims='z')\n",
    "lons_ = xr.DataArray(list(data['nearest_lon'].values), dims='z')\n",
    "\n",
    "for feature in tqdm(ten_day_features):\n",
    "    with xr.open_dataset(ten_day_data_folder + ten_day_feature_files[feature]) as ds:\n",
    "        feature_data = ds.load()\n",
    "    for day in days:\n",
    "        for month in months:\n",
    "            for year in years:\n",
    "                time = f'{year}-{month}-{day}'\n",
    "                time_data = feature_data.sel(time=time).squeeze().sel(lat=lats_).sel(lon=lons_)\n",
    "                data[f'{feature}-{month}-{day}-{year}'] = getattr(time_data, feature)\n",
    "    del feature_data\n",
    "    del time_data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a csv\n",
    "file_path = f'head_of_soils_recommendations.csv'\n",
    "data.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a CSV file\n",
    "file_path = f'head_of_soils_recommendations.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a custom train and test set adding in the features you want. It reads an already computed \n",
    "# dataset but you can then remove features to see the effect on score.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Which data set are we reading in?\n",
    "read_file_path = f'head_of_soils_recommendations.csv'\n",
    "\n",
    "# Set the random seed for the whole process of creating the training and test set.\n",
    "random_seed = 42\n",
    "random.seed(a=random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Test set fraction. What fraction of the data set to make a test set\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "# Only read a sample fraction of the data, speeds things up, but you'll get worse performance.\n",
    "# Use 1.0 as the sample fraction to get the full data set\n",
    "sample_fraction = 1.0\n",
    "data = pd.read_csv(read_file_path, skiprows=lambda i: i>0 and random.random() > sample_fraction)\n",
    "\n",
    "# Remove datapoints where yield changes by more than the fraction between 2005 and 2010.\n",
    "# This is unlikely to be due to changes in climate.\n",
    "limit_change_in_yield = True\n",
    "limit = 1.0\n",
    "\n",
    "# Which type of features to include (if True must already exist in read dataset)\n",
    "include_coordinates = True\n",
    "include_2005_maize = True\n",
    "include_seasonal = True\n",
    "include_ten_day = True\n",
    "include_2005_climate = True\n",
    "include_soil_type = False\n",
    "include_growing_zones = False\n",
    "\n",
    "# Drop nans?\n",
    "drop_nans = True\n",
    "\n",
    "\n",
    "# Choose a subset of your features! (must already exist in original file)\n",
    "ten_day_features = [\n",
    "    'BEDD',\n",
    "    'R20mm',\n",
    "    'DTR'\n",
    "]\n",
    "\n",
    "seasonal_features = [\n",
    "    'CDD',\n",
    "    'CFD',\n",
    "    'CWD',\n",
    "    'WW',\n",
    "    'WSDI',\n",
    "    'CSDI'\n",
    "]\n",
    "\n",
    "if limit_change_in_yield:\n",
    "    data = data[abs(data['maize_a_2010'] - data['maize_a_2005']) / data['maize_a_2005'] < limit]\n",
    "\n",
    "features = []\n",
    "\n",
    "if include_growing_zones:\n",
    "    growing_zones = [\n",
    "        'Inland water bodies',\n",
    "        'Subtropics - summer rainfall',\n",
    "        'Subtropics - winter rainfall',\n",
    "        'Temperature - continental',\n",
    "        'Temperature - oceanic',\n",
    "        'Temperature - subcontinental',\n",
    "        'Tropics'\n",
    "    ]\n",
    "    \n",
    "if include_soil_type:\n",
    "    features = features + ['soil_type']\n",
    "\n",
    "if include_2005_climate:\n",
    "    years = ['2010', '2005']\n",
    "else:\n",
    "    years = ['2010']\n",
    "\n",
    "if include_seasonal:\n",
    "    quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "    for feature in seasonal_features:\n",
    "        for season in seasons:\n",
    "            for year in years:\n",
    "                features.append(f'{feature}-{quarter}-{year}')\n",
    "\n",
    "if include_ten_day:\n",
    "    days = ['05', '15', '25']\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "    for feature in ten_day_features:\n",
    "        for day in days:\n",
    "            for month in months:\n",
    "                for year in years:\n",
    "                    features.append(f'{feature}-{month}-{day}-{year}')\n",
    "\n",
    "if include_coordinates:\n",
    "    features = features + ['lon', 'lat']\n",
    "    \n",
    "if include_2005_maize:\n",
    "    features = features + ['maize_a_2005']\n",
    "\n",
    "target = 'maize_a_2010'\n",
    "\n",
    "if drop_nans:\n",
    "    data = data.dropna()\n",
    "    \n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "del X\n",
    "del y\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a simple random forest regressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, n_jobs=10, random_state=random_seed)\n",
    "regressor.fit(X_train, y_train)\n",
    "score = regressor.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Hadgem data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
